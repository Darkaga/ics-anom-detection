# Phase 4 README Section - Add to Main README

## Phase 4: Real-Time Detection & REST API ðŸš€

Real-time anomaly detection with REST API for monitoring and querying.

### Features

- **Real-time monitoring** of live Modbus traffic
- **5-minute sliding windows** for feature extraction
- **28 behavioral features** (same as training)
- **Isolation Forest** anomaly detection
- **REST API** with FastAPI
- **Parquet storage** for efficient queries
- **In-memory buffer** for instant anomaly access

### Quick Start

```bash
# Build detection API container
make detection-build

# Start real-time detection (requires trained model from Phase 3)
make detection-up

# Check status
curl http://localhost:8000/status | jq

# View recent anomalies
curl http://localhost:8000/anomalies/current | jq

# Monitor logs
make detection-logs
```

### API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/health` | GET | Health check |
| `/status` | GET | System status & metrics |
| `/anomalies/current` | GET | Recent anomalies (in-memory, last 100) |
| `/anomalies/history` | POST | Query historical data with filters |
| `/anomalies/stats` | GET | Detection statistics |
| `/info/features` | GET | Feature information |
| `/docs` | GET | Interactive API documentation (Swagger) |

### Example Usage

```bash
# Get system status
curl http://localhost:8000/status

# Get last 20 anomalies
curl http://localhost:8000/anomalies/current?limit=20

# Query historical anomalies
curl -X POST http://localhost:8000/anomalies/history \
  -H "Content-Type: application/json" \
  -d '{
    "start_time": "2025-11-08T00:00:00",
    "src": "192.168.0.21",
    "limit": 50
  }'

# Get detection statistics
curl http://localhost:8000/anomalies/stats
```

### Python Client Example

```python
import requests
import pandas as pd

# Check system status
response = requests.get('http://localhost:8000/status')
status = response.json()
print(f"Records processed: {status['records_processed']}")
print(f"Anomalies detected: {status['anomalies_detected']}")

# Get recent anomalies
response = requests.get('http://localhost:8000/anomalies/current?limit=10')
anomalies = response.json()['anomalies']

for anom in anomalies:
    print(f"Anomaly: {anom['src']} â†’ {anom['dst']}, score: {anom['anomaly_score']:.3f}")

# Query historical data
query = {
    "start_time": "2025-11-08T00:00:00",
    "dst": "192.168.0.11",
    "min_score": -1.0,
    "limit": 100
}
response = requests.post('http://localhost:8000/anomalies/history', json=query)
historical_df = pd.DataFrame(response.json()['anomalies'])
```

### How It Works

1. **Log Monitoring** (every 5 seconds):
   - Tails `modbus_detailed-current.log`
   - Parses JSON records
   - Buffers in 5-minute windows

2. **Feature Extraction** (every 5 minutes):
   - Extracts 28 behavioral features
   - Aggregates register â†’ device pairs
   - Adds temporal context (rolling baselines)

3. **Anomaly Detection** (immediate):
   - Scales features using trained scaler
   - Applies Isolation Forest model
   - Flags anomalies (prediction = -1)
   - Saves to Parquet files
   - Updates in-memory buffer

### Configuration

Environment variables (set in `compose/compose.detection.yaml`):

```yaml
environment:
  - LOG_FILE=/data/zeek/modbus_detailed-current.log
  - MODEL_PATH=/data/models/anomaly_detector.pkl
  - OUTPUT_DIR=/data/detections
  - WINDOW_SECONDS=300        # 5-minute windows
  - POLL_INTERVAL=5           # Check every 5 seconds
  - ANOMALY_THRESHOLD=-0.5    # Score threshold
  - LOG_LEVEL=INFO
```

### Output Files

Detection results saved to `data/detections/`:

```
anomalies_20251108_103000.parquet
anomalies_20251108_104500.parquet
...
```

Each file contains:
- Time window
- Source/destination IPs
- Anomaly score
- All 28 feature values
- Detection timestamp

### Commands

```bash
# Core commands
make detection-build        # Build container
make detection-up           # Start API
make detection-down         # Stop API
make detection-restart      # Restart API

# Monitoring
make detection-logs         # View logs
make detection-status       # Quick status check
make detection-test         # Test all endpoints

# Maintenance
make detection-clean        # Clean output files
```

### Validation

```bash
# Run validation script
./scripts/validate_phase4.sh

# Expected output:
# âœ“ API is running
# âœ“ Health check passed
# âœ“ Detector is running
# âœ“ Correct feature count (28)
# âœ“ Statistics retrieved
```

### Performance

- **Latency:** ~5 seconds (poll interval)
- **Throughput:** Processes ~4,000 transactions/minute
- **Window Size:** 5 minutes (configurable)
- **Feature Extraction:** ~1 second per window
- **Model Inference:** ~0.2 seconds per window
- **Memory Usage:** ~500MB
- **Storage:** ~1MB/hour (Parquet, compressed)

### Troubleshooting

**Problem:** "Detector not initialized"
```bash
# Train model first
make ml-pipeline
```

**Problem:** No anomalies detected
```bash
# Check if data is flowing
ls -lh data/zeek/modbus_detailed-current.log
make detection-logs

# Wait 10-15 minutes for multiple windows
```

**Problem:** High false positives
```bash
# Adjust threshold in compose/compose.detection.yaml
# Make more strict (e.g., -0.7 instead of -0.5)
make detection-restart
```

### Documentation

- **Quick Start:** `docs/PHASE4-QUICKSTART.md`
- **Full Documentation:** `docs/phase4-detection.md`
- **API Documentation:** http://localhost:8000/docs (when running)

### Next Steps

âœ… Phase 4 gives you real-time detection with a REST API

**Ready for Phase 5:** Build a Streamlit dashboard to visualize:
- Real-time anomaly feed
- System topology
- Register value trends
- Detection timeline
- Performance metrics
