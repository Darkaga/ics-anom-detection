#!/usr/bin/env python3
"""
GPU-Accelerated ML Training for ICS Anomaly Detection
Trains anomaly detection models using RAPIDS cuML
"""

import cudf
import cupy as cp
from cuml.ensemble import IsolationForest, RandomForestClassifier
from cuml.cluster import DBSCAN
from cuml.preprocessing import StandardScaler
import argparse
import joblib
import logging
from pathlib import Path
import sys
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_features(features_path):
    """
    Load extracted features from CSV
    
    Args:
        features_path: Path to features CSV file
    
    Returns:
        cudf.DataFrame with features
    """
    logger.info(f"Loading features from {features_path}...")
    df = cudf.read_csv(features_path)
    logger.info(f"Loaded {len(df):,} feature vectors with {df.shape[1]} dimensions")
    
    return df


def prepare_training_data(df, label_col=None):
    """
    Prepare features for training
    
    Args:
        df: cudf.DataFrame with features
        label_col: Name of label column (if supervised learning)
    
    Returns:
        X: Feature matrix
        y: Labels (if supervised)
        feature_names: List of feature column names
    """
    logger.info("Preparing training data...")
    
    # Identify metadata columns to exclude
    metadata_cols = ['time_window', 'id.orig_h', 'id.resp_h', 'uid', 'timestamp']
    metadata_cols = [col for col in metadata_cols if col in df.columns]
    
    # Exclude metadata and label columns
    exclude_cols = metadata_cols.copy()
    if label_col and label_col in df.columns:
        exclude_cols.append(label_col)
    
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    logger.info(f"Using {len(feature_cols)} features for training")
    logger.info(f"Excluded {len(exclude_cols)} metadata columns")
    
    X = df[feature_cols]
    y = df[label_col] if label_col and label_col in df.columns else None
    
    return X, y, feature_cols


def train_isolation_forest(X, contamination=0.01, n_estimators=100):
    """
    Train Isolation Forest for anomaly detection
    
    Args:
        X: Feature matrix
        contamination: Expected proportion of anomalies
        n_estimators: Number of trees
    
    Returns:
        Trained model and predictions
    """
    logger.info("Training Isolation Forest...")
    logger.info(f"  Contamination: {contamination}")
    logger.info(f"  N estimators: {n_estimators}")
    
    model = IsolationForest(
        contamination=contamination,
        n_estimators=n_estimators,
        random_state=42
    )
    
    model.fit(X)
    predictions = model.predict(X)
    scores = model.score_samples(X)
    
    # Convert predictions: -1 (anomaly) to 1, 1 (normal) to 0
    anomalies = (predictions == -1).astype('int32')
    n_anomalies = anomalies.sum()
    
    logger.info(f"Training complete: {n_anomalies} anomalies detected ({n_anomalies/len(X)*100:.2f}%)")
    
    return model, anomalies, scores


def train_dbscan_clustering(X, eps=0.5, min_samples=5):
    """
    Train DBSCAN for clustering-based anomaly detection
    
    Args:
        X: Feature matrix
        eps: Maximum distance between samples
        min_samples: Minimum samples in a neighborhood
    
    Returns:
        Trained model and cluster assignments
    """
    logger.info("Training DBSCAN clustering...")
    logger.info(f"  Eps: {eps}")
    logger.info(f"  Min samples: {min_samples}")
    
    # Normalize features first
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    model = DBSCAN(
        eps=eps,
        min_samples=min_samples
    )
    
    clusters = model.fit_predict(X_scaled)
    
    # Points with cluster -1 are anomalies
    anomalies = (clusters == -1).astype('int32')
    n_anomalies = anomalies.sum()
    n_clusters = len(cp.unique(clusters[clusters != -1]))
    
    logger.info(f"Clustering complete: {n_clusters} clusters, {n_anomalies} outliers ({n_anomalies/len(X)*100:.2f}%)")
    
    return model, anomalies, clusters, scaler


def evaluate_model(y_true, y_pred, model_name="Model"):
    """
    Evaluate anomaly detection model
    
    Args:
        y_true: True labels (if available)
        y_pred: Predicted labels
        model_name: Name of the model
    """
    if y_true is None:
        logger.info(f"{model_name} - Unsupervised mode, no ground truth for evaluation")
        return
    
    from cuml.metrics import accuracy_score, confusion_matrix
    
    accuracy = accuracy_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)
    
    logger.info(f"{model_name} Evaluation:")
    logger.info(f"  Accuracy: {accuracy:.4f}")
    logger.info(f"  Confusion Matrix:\n{cm}")


def save_model(model, scaler, feature_names, output_path, model_name, metadata=None):
    """
    Save trained model and associated artifacts
    
    Args:
        model: Trained model
        scaler: Feature scaler (if used)
        feature_names: List of feature names
        output_path: Output directory
        model_name: Name for saving
        metadata: Additional metadata dict
    """
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_file = output_dir / f"{model_name}.pkl"
    logger.info(f"Saving model to {model_file}...")
    
    model_package = {
        'model': model,
        'scaler': scaler,
        'feature_names': feature_names,
        'metadata': metadata or {}
    }
    
    joblib.dump(model_package, model_file)
    
    # Save feature names separately
    feature_file = output_dir / f"{model_name}_features.json"
    with open(feature_file, 'w') as f:
        json.dump(feature_names, f, indent=2)
    
    logger.info(f"Model saved successfully")


def save_results(df, predictions, scores, output_path, result_name):
    """
    Save prediction results
    
    Args:
        df: Original dataframe with metadata
        predictions: Anomaly predictions
        scores: Anomaly scores
        output_path: Output directory
        result_name: Name for results file
    """
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Add predictions to dataframe
    results_df = df.copy()
    results_df['anomaly'] = predictions
    results_df['anomaly_score'] = scores
    
    output_file = output_dir / f"{result_name}_predictions.csv"
    logger.info(f"Saving results to {output_file}...")
    results_df.to_csv(output_file, index=False)
    
    # Save summary statistics
    summary_file = output_dir / f"{result_name}_summary.json"
    summary = {
        'total_samples': len(results_df),
        'anomalies_detected': int(predictions.sum()),
        'anomaly_rate': float(predictions.sum() / len(results_df)),
        'mean_anomaly_score': float(scores.mean()),
        'std_anomaly_score': float(scores.std())
    }
    
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    logger.info(f"Results saved: {summary['anomalies_detected']} anomalies detected")


def main():
    parser = argparse.ArgumentParser(
        description='Train anomaly detection models using GPU acceleration'
    )
    parser.add_argument(
        'features_path',
        help='Path to features CSV file'
    )
    parser.add_argument(
        '--output',
        default='/workspace/data/models',
        help='Output directory for models'
    )
    parser.add_argument(
        '--model',
        choices=['isolation_forest', 'dbscan', 'all'],
        default='all',
        help='Model to train (default: all)'
    )
    parser.add_argument(
        '--contamination',
        type=float,
        default=0.01,
        help='Expected contamination rate for Isolation Forest (default: 0.01)'
    )
    parser.add_argument(
        '--n-estimators',
        type=int,
        default=100,
        help='Number of estimators for Isolation Forest (default: 100)'
    )
    parser.add_argument(
        '--eps',
        type=float,
        default=0.5,
        help='Epsilon for DBSCAN (default: 0.5)'
    )
    parser.add_argument(
        '--min-samples',
        type=int,
        default=5,
        help='Minimum samples for DBSCAN (default: 5)'
    )
    
    args = parser.parse_args()
    
    try:
        logger.info("="*70)
        logger.info("GPU-Accelerated ML Training Starting")
        logger.info("="*70)
        
        # Load features
        df = load_features(args.features_path)
        X, y, feature_names = prepare_training_data(df)
        
        # Train models
        if args.model in ['isolation_forest', 'all']:
            logger.info("\n" + "="*70)
            logger.info("Training Isolation Forest")
            logger.info("="*70)
            
            if_model, if_predictions, if_scores = train_isolation_forest(
                X,
                contamination=args.contamination,
                n_estimators=args.n_estimators
            )
            
            save_model(
                if_model, None, feature_names, args.output,
                'isolation_forest',
                metadata={
                    'contamination': args.contamination,
                    'n_estimators': args.n_estimators
                }
            )
            
            save_results(df, if_predictions, if_scores, args.output, 'isolation_forest')
        
        if args.model in ['dbscan', 'all']:
            logger.info("\n" + "="*70)
            logger.info("Training DBSCAN")
            logger.info("="*70)
            
            dbscan_model, dbscan_predictions, clusters, scaler = train_dbscan_clustering(
                X,
                eps=args.eps,
                min_samples=args.min_samples
            )
            
            save_model(
                dbscan_model, scaler, feature_names, args.output,
                'dbscan',
                metadata={
                    'eps': args.eps,
                    'min_samples': args.min_samples
                }
            )
            
            save_results(df, dbscan_predictions, clusters, args.output, 'dbscan')
        
        logger.info("\n" + "="*70)
        logger.info("Training Complete!")
        logger.info("="*70)
        logger.info(f"Models saved to: {args.output}")
        
        return 0
        
    except Exception as e:
        logger.error(f"Training failed: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
