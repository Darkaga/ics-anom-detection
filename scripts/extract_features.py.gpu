#!/usr/bin/env python3
"""
GPU-Accelerated Feature Extraction for ICS Anomaly Detection
Extracts temporal, statistical, and behavioral features from Zeek Modbus logs
"""

import cudf
import cupy as cp
import argparse
import glob
import logging
from pathlib import Path
import sys

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_zeek_logs(log_pattern):
    """
    Load Zeek Modbus logs using GPU-accelerated cuDF
    
    Args:
        log_pattern: Glob pattern for log files (e.g., /workspace/data/zeek/modbus*.log)
    
    Returns:
        cudf.DataFrame with all logs combined
    """
    log_files = glob.glob(log_pattern)
    
    if not log_files:
        raise ValueError(f"No log files found matching pattern: {log_pattern}")
    
    logger.info(f"Found {len(log_files)} log files to process")
    
    dfs = []
    for log_file in sorted(log_files):
        logger.info(f"Loading {log_file}...")
        try:
            # Read JSON lines format
            df = cudf.read_json(log_file, lines=True)
            logger.info(f"  Loaded {len(df):,} records")
            dfs.append(df)
        except Exception as e:
            logger.error(f"  Error loading {log_file}: {e}")
            continue
    
    if not dfs:
        raise ValueError("No valid log files could be loaded")
    
    # Combine all dataframes
    combined_df = cudf.concat(dfs, ignore_index=True)
    logger.info(f"Total records loaded: {len(combined_df):,}")
    
    return combined_df


def extract_time_window_features(df, window_seconds=60):
    """
    Extract features aggregated over time windows
    
    Args:
        df: cudf.DataFrame with Zeek logs
        window_seconds: Size of time window in seconds
    
    Returns:
        cudf.DataFrame with time-windowed features
    """
    logger.info(f"Extracting time-window features (window={window_seconds}s)...")
    
    # Convert timestamp to datetime
    df['timestamp'] = cudf.to_datetime(df['ts'], unit='s')
    
    # Create time windows
    df['time_window'] = (df['ts'] // window_seconds).astype('int64')
    
    # Group by time window and source/destination pairs
    grouped = df.groupby(['time_window', 'id.orig_h', 'id.resp_h'])
    
    features = grouped.agg({
        'ts': ['count', 'min', 'max'],  # Transaction counts and timing
        'tid': 'nunique',                # Unique transaction IDs
        'unit': 'nunique',               # Unique unit IDs
        'id.orig_p': 'nunique',          # Source port diversity
    })
    
    # Flatten column names
    features.columns = ['_'.join(col).strip() for col in features.columns.values]
    features = features.reset_index()
    
    # Calculate derived features
    features['duration'] = features['ts_max'] - features['ts_min']
    features['transaction_rate'] = features['ts_count'] / window_seconds
    
    # Add function code distribution per window
    func_counts = df.groupby(['time_window', 'id.orig_h', 'id.resp_h', 'func']).size()
    func_pivot = func_counts.reset_index().pivot_table(
        index=['time_window', 'id.orig_h', 'id.resp_h'],
        columns='func',
        values=0,
        fill_value=0
    )
    func_pivot.columns = [f'func_{col}_count' for col in func_pivot.columns]
    func_pivot = func_pivot.reset_index()
    
    # Merge function counts
    features = features.merge(
        func_pivot,
        on=['time_window', 'id.orig_h', 'id.resp_h'],
        how='left'
    )
    
    logger.info(f"Extracted {len(features):,} time-window feature vectors")
    logger.info(f"Feature dimensions: {features.shape[1]} features")
    
    return features


def extract_connection_features(df):
    """
    Extract per-connection behavioral features
    
    Args:
        df: cudf.DataFrame with Zeek logs
    
    Returns:
        cudf.DataFrame with connection-level features
    """
    logger.info("Extracting connection-level features...")
    
    # Group by unique connection (uid)
    conn_grouped = df.groupby('uid')
    
    conn_features = conn_grouped.agg({
        'ts': ['count', 'min', 'max'],
        'tid': 'nunique',
        'func': 'nunique',
        'pdu_type': lambda x: (x == 'REQ').sum(),  # Count requests
    })
    
    conn_features.columns = ['_'.join(col).strip() for col in conn_features.columns.values]
    conn_features = conn_features.reset_index()
    
    # Rename for clarity
    conn_features = conn_features.rename(columns={
        'ts_count': 'transaction_count',
        'ts_min': 'conn_start',
        'ts_max': 'conn_end',
        'tid_nunique': 'unique_tids',
        'func_nunique': 'unique_functions',
        'pdu_type_<lambda>': 'request_count'
    })
    
    conn_features['conn_duration'] = conn_features['conn_end'] - conn_features['conn_start']
    conn_features['response_count'] = conn_features['transaction_count'] - conn_features['request_count']
    conn_features['req_resp_ratio'] = conn_features['request_count'] / (conn_features['response_count'] + 1)
    
    logger.info(f"Extracted {len(conn_features):,} connection feature vectors")
    
    return conn_features


def extract_statistical_features(df, window_seconds=60):
    """
    Extract statistical features (mean, std, percentiles) over time windows
    
    Args:
        df: cudf.DataFrame with Zeek logs
        window_seconds: Size of time window
    
    Returns:
        cudf.DataFrame with statistical features
    """
    logger.info("Extracting statistical features...")
    
    df['time_window'] = (df['ts'] // window_seconds).astype('int64')
    
    # Calculate inter-arrival times
    df = df.sort_values(['id.orig_h', 'id.resp_h', 'ts'])
    df['inter_arrival_time'] = df.groupby(['id.orig_h', 'id.resp_h'])['ts'].diff()
    
    grouped = df.groupby(['time_window', 'id.orig_h', 'id.resp_h'])
    
    # Statistical aggregations
    stats_features = grouped['inter_arrival_time'].agg([
        'mean', 'std', 'min', 'max',
        ('p25', lambda x: x.quantile(0.25)),
        ('p50', lambda x: x.quantile(0.50)),
        ('p75', lambda x: x.quantile(0.75)),
        ('p95', lambda x: x.quantile(0.95))
    ]).reset_index()
    
    stats_features.columns = ['time_window', 'id.orig_h', 'id.resp_h',
                               'iat_mean', 'iat_std', 'iat_min', 'iat_max',
                               'iat_p25', 'iat_p50', 'iat_p75', 'iat_p95']
    
    # Fill NaN values
    stats_features = stats_features.fillna(0)
    
    logger.info(f"Extracted {len(stats_features):,} statistical feature vectors")
    
    return stats_features


def normalize_features(features_df, output_path):
    """
    Normalize features using GPU-accelerated StandardScaler
    
    Args:
        features_df: cudf.DataFrame with raw features
        output_path: Path to save normalized features
    
    Returns:
        cudf.DataFrame with normalized features
    """
    from cuml.preprocessing import StandardScaler
    
    logger.info("Normalizing features...")
    
    # Separate numeric and non-numeric columns
    numeric_cols = features_df.select_dtypes(include=[cp.number]).columns.tolist()
    non_numeric_cols = [col for col in features_df.columns if col not in numeric_cols]
    
    # Initialize scaler
    scaler = StandardScaler()
    
    # Fit and transform numeric features
    features_numeric = features_df[numeric_cols]
    features_normalized = scaler.fit_transform(features_numeric)
    
    # Convert back to DataFrame
    normalized_df = cudf.DataFrame(features_normalized, columns=numeric_cols)
    
    # Add back non-numeric columns
    for col in non_numeric_cols:
        normalized_df[col] = features_df[col].reset_index(drop=True)
    
    logger.info("Features normalized")
    
    return normalized_df


def main():
    parser = argparse.ArgumentParser(
        description='Extract features from Zeek Modbus logs using GPU acceleration'
    )
    parser.add_argument(
        'log_pattern',
        help='Glob pattern for Zeek log files (e.g., /workspace/data/zeek/modbus*.log)'
    )
    parser.add_argument(
        '--output',
        default='/workspace/data/features',
        help='Output directory for features'
    )
    parser.add_argument(
        '--window',
        type=int,
        default=60,
        help='Time window in seconds (default: 60)'
    )
    parser.add_argument(
        '--normalize',
        action='store_true',
        help='Normalize features'
    )
    
    args = parser.parse_args()
    
    try:
        # Create output directory
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("="*70)
        logger.info("GPU-Accelerated Feature Extraction Starting")
        logger.info("="*70)
        
        # Load logs
        df = load_zeek_logs(args.log_pattern)
        
        # Extract different feature sets
        time_features = extract_time_window_features(df, args.window)
        conn_features = extract_connection_features(df)
        stats_features = extract_statistical_features(df, args.window)
        
        # Merge all features
        logger.info("Merging feature sets...")
        all_features = time_features.merge(
            stats_features,
            on=['time_window', 'id.orig_h', 'id.resp_h'],
            how='left'
        )
        all_features = all_features.fillna(0)
        
        # Normalize if requested
        if args.normalize:
            all_features = normalize_features(all_features, args.output)
        
        # Save features
        time_window_output = output_dir / 'features_time_windows.csv'
        connection_output = output_dir / 'features_connections.csv'
        
        logger.info(f"Saving time-window features to {time_window_output}...")
        all_features.to_csv(time_window_output, index=False)
        
        logger.info(f"Saving connection features to {connection_output}...")
        conn_features.to_csv(connection_output, index=False)
        
        logger.info("="*70)
        logger.info("Feature Extraction Complete!")
        logger.info("="*70)
        logger.info(f"Time-window features: {len(all_features):,} vectors, {all_features.shape[1]} dimensions")
        logger.info(f"Connection features: {len(conn_features):,} vectors, {conn_features.shape[1]} dimensions")
        logger.info(f"Output directory: {args.output}")
        
        return 0
        
    except Exception as e:
        logger.error(f"Feature extraction failed: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
